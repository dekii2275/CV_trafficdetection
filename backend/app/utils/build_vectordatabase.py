"""
Script Ä‘á»ƒ build vector database tá»« cÃ¡c file luáº­t giao thÃ´ng.
ÄÃ£ sá»­a lá»—i Import LangChain vÃ  lá»—i tham sá»‘ dÃ²ng lá»‡nh.

Usage:
    python3 app/utils/build_vectordatabase.py --reset
"""

import sys
import os
import re
import shutil
from typing import List
from pathlib import Path
import docx  # YÃªu cáº§u: pip install python-docx

# --- 1. Sá»¬A Lá»–I IMPORT LANGCHAIN ---
# Tá»± Ä‘á»™ng detect phiÃªn báº£n LangChain Ä‘á»ƒ import Ä‘Ãºng
try:
    # DÃ nh cho phiÃªn báº£n LangChain má»›i (v0.1+)
    from langchain_core.documents import Document
except ImportError:
    try:
        # DÃ nh cho phiÃªn báº£n cÅ© hÆ¡n
        from langchain.schema import Document
    except ImportError:
        # Fallback cuá»‘i cÃ¹ng
        from langchain.docstore.document import Document

# --- 2. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN TUYá»†T Äá»I ---
# Láº¥y vá»‹ trÃ­ thá»±c táº¿ cá»§a file nÃ y
FILE_PATH = Path(__file__).resolve()
# Cáº¥u trÃºc thÆ° má»¥c: .../CV_trafficdetection/backend/app/utils/build_vectordatabase.py
# Parents: [0]=utils, [1]=app, [2]=backend, [3]=CV_trafficdetection (Project Root)
PROJECT_ROOT = FILE_PATH.parents[3] 
BACKEND_ROOT = FILE_PATH.parents[2] 

# ThÃªm backend vÃ o sys.path Ä‘á»ƒ Python tÃ¬m tháº¥y cÃ¡c module ná»™i bá»™ (nhÆ° app.services...)
sys.path.append(str(BACKEND_ROOT))

# Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n Data (Tuyá»‡t Ä‘á»‘i)
ABS_DOCS_DIR = PROJECT_ROOT / "data" / "law_documents"
ABS_DB_DIR = PROJECT_ROOT / "data" / "chroma_db"

# Import service cá»§a báº¡n
try:
    from app.services.rag_services.vector_store import VectorStoreService
except ImportError as e:
    print(f"âŒ Lá»—i Import Service: {e}")
    print(f"ğŸ‘‰ Äáº£m báº£o báº¡n Ä‘ang Ä‘á»©ng á»Ÿ thÆ° má»¥c 'backend' vÃ  file vector_store.py tá»“n táº¡i.")
    sys.exit(1)

# ============================================================
# 3. CLASS Xá»¬ LÃ VÄ‚N Báº¢N LUáº¬T (LOGIC CHIA NHá»)
# ============================================================
class TrafficLawProcessor:
    """
    Xá»­ lÃ½ vÄƒn báº£n luáº­t: TÃ¡ch Äiá»u -> Khoáº£n -> Äiá»ƒm Ä‘á»ƒ trÃ¡nh máº¥t thÃ´ng tin
    """
    def __init__(self):
        # Regex tÃ¬m "Äiá»u X."
        self.article_pattern = r"(^|\n)(Äiá»u \d+\..*?)(?=\nÄiá»u \d+\.|$)"
        # Regex tÃ¬m "1. ", "2. " (Khoáº£n)
        self.clause_pattern = r"(^|\n)(\d+)\.\s+(.*?)(?=(\n\d+\.\s+)|$)"
        # Regex tÃ¬m "a) ", "b) ", "Ä‘) " (Äiá»ƒm)
        self.point_pattern = r"(^|\n)([a-zÄ‘])\)\s+(.*?)(?=(\n[a-zÄ‘]\))|$)" 
    
    def read_docx(self, file_path: str) -> str:
        """Äá»c file .docx vÃ  chuyá»ƒn thÃ nh string"""
        try:
            doc = docx.Document(file_path)
            full_text = []
            for para in doc.paragraphs:
                txt = para.text.strip()
                if txt:
                    full_text.append(txt)
            return "\n".join(full_text)
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file {file_path}: {e}")
            return ""

    def identify_vehicle_type(self, text: str) -> str:
        """Nháº­n diá»‡n loáº¡i xe tá»« tiÃªu Ä‘á» Äiá»u luáº­t"""
        text_lower = text.lower()
        if "xe Ã´ tÃ´" in text_lower: return "oto"
        if "xe mÃ´ tÃ´" in text_lower or "xe gáº¯n mÃ¡y" in text_lower: return "xemay"
        if "xe Ä‘áº¡p" in text_lower or "xe thÃ´ sÆ¡" in text_lower: return "xedap"
        if "ngÆ°á»i Ä‘i bá»™" in text_lower: return "nguoidibo"
        return "chung"

    def process_document(self, file_path: str) -> List[Document]:
        text = self.read_docx(file_path)
        if not text: return []
        
        chunks = []
        source_name = Path(file_path).name
        
        # B1: TÃ¡ch cÃ¡c Äiá»u (Articles)
        articles = re.finditer(self.article_pattern, text, re.DOTALL)
        
        for art_match in articles:
            article_full_text = art_match.group(2)
            # Láº¥y dÃ²ng Ä‘áº§u tiÃªn lÃ m tiÃªu Ä‘á» (VD: Äiá»u 5. Xá»­ pháº¡t...)
            article_header = article_full_text.strip().split('\n')[0]
            vehicle_type = self.identify_vehicle_type(article_header)
            
            # B2: TÃ¡ch cÃ¡c Khoáº£n (Clauses) trong Äiá»u
            clauses = re.finditer(self.clause_pattern, article_full_text, re.DOTALL)
            has_clauses = False
            
            for clause_match in clauses:
                has_clauses = True
                clause_num = clause_match.group(2)
                clause_body = clause_match.group(3).strip()
                
                # B3: TÃ¡ch cÃ¡c Äiá»ƒm (Points: a, b, c...) trong Khoáº£n
                points = list(re.finditer(self.point_pattern, clause_body, re.DOTALL))
                
                if points:
                    # Láº¥y pháº§n dáº«n nháº­p (VD: "Pháº¡t tiá»n tá»« 200k... hÃ nh vi sau:")
                    intro_text = clause_body[:points[0].start()].strip()
                    
                    for p_match in points:
                        p_label = p_match.group(2) # a, b, c
                        p_content = p_match.group(3).strip()
                        
                        # Táº¡o ná»™i dung Chunk chi tiáº¿t
                        full_content = (
                            f"ÄIá»€U LUáº¬T: {article_header}\n"
                            f"Má»¨C PHáº T (Khoáº£n {clause_num}): {intro_text}\n"
                            f"HÃ€NH VI VI PHáº M (Äiá»ƒm {p_label}): {p_content}"
                        )
                        
                        chunks.append(Document(
                            page_content=full_content,
                            metadata={
                                "source": source_name,
                                "article": article_header.split('.')[0], # VD: Äiá»u 5
                                "vehicle": vehicle_type,
                                "level": "point" # Cáº¥p Ä‘á»™ chi tiáº¿t nháº¥t
                            }
                        ))
                else:
                    # Náº¿u khÃ´ng cÃ³ Ä‘iá»ƒm a,b,c -> Láº¥y nguyÃªn Khoáº£n
                    full_content = (
                        f"ÄIá»€U LUáº¬T: {article_header}\n"
                        f"Ná»˜I DUNG (Khoáº£n {clause_num}): {clause_body}"
                    )
                    chunks.append(Document(
                        page_content=full_content,
                        metadata={
                            "source": source_name,
                            "article": article_header.split('.')[0],
                            "vehicle": vehicle_type,
                            "level": "clause"
                        }
                    ))

            # Náº¿u Äiá»u quÃ¡ ngáº¯n khÃ´ng cÃ³ khoáº£n (chá»‰ cÃ³ text)
            if not has_clauses:
                chunks.append(Document(
                    page_content=article_full_text, 
                    metadata={"source": source_name, "vehicle": vehicle_type, "level": "article"}
                ))
                
        return chunks

# ============================================================
# 4. HÃ€M CHÃNH: BUILD DATABASE
# ============================================================
def build_vector_database(documents_dir: str = str(ABS_DOCS_DIR), reset: bool = False):
    print("\n" + "="*60)
    print("ğŸš€ RAG BUILDER: SMART CHUNKING (Luáº­t Giao ThÃ´ng)")
    print("="*60)
    print(f"ğŸ“‚ Äá»c tÃ i liá»‡u tá»«: {documents_dir}")
    print(f"ğŸ“‚ LÆ°u Database táº¡i: {ABS_DB_DIR}")
    
    # Xá»­ lÃ½ tham sá»‘ Reset
    if reset:
        if ABS_DB_DIR.exists():
            print(f"ğŸ—‘ï¸  Äang xÃ³a database cÅ© Ä‘á»ƒ lÃ m sáº¡ch dá»¯ liá»‡u...")
            shutil.rmtree(ABS_DB_DIR)
        else:
            print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y database cÅ©, sáº½ táº¡o má»›i hoÃ n toÃ n.")
    
    # Init Vector Store
    print(f"ğŸ“¦ Äang khá»Ÿi táº¡o Vector Store...")
    vector_store = VectorStoreService(
        collection_name="traffic_laws",
        persist_directory=str(ABS_DB_DIR)
    )

    # Xá»­ lÃ½ file
    processor = TrafficLawProcessor()
    all_documents = []
    
    if not os.path.exists(documents_dir):
        print(f"âŒ Lá»–I: ThÆ° má»¥c tÃ i liá»‡u khÃ´ng tá»“n táº¡i: {documents_dir}")
        print(f"ğŸ‘‰ Vui lÃ²ng táº¡o thÆ° má»¥c nÃ y vÃ  copy file .docx vÃ o Ä‘Ã³.")
        return

    files = [f for f in os.listdir(documents_dir) if f.endswith(".docx") or f.endswith(".doc")]
    if not files:
        print(f"âš ï¸  Cáº¢NH BÃO: ThÆ° má»¥c {documents_dir} trá»‘ng! HÃ£y copy file luáº­t vÃ o.")
        return

    for filename in files:
        file_path = os.path.join(documents_dir, filename)
        print(f"\nğŸ“„ Äang xá»­ lÃ½ file: {filename}...")
        
        chunks = processor.process_document(file_path)
        all_documents.extend(chunks)
        print(f"   -> Táº¡o Ä‘Æ°á»£c {len(chunks)} chunks dá»¯ liá»‡u.")

    if not all_documents:
        print("âš ï¸ KhÃ´ng táº¡o Ä‘Æ°á»£c dá»¯ liá»‡u nÃ o. Kiá»ƒm tra láº¡i ná»™i dung file input.")
        return

    # LÆ°u vÃ o ChromaDB
    print(f"\nğŸ’¾ Äang lÆ°u {len(all_documents)} chunks vÃ o Database (QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
    
    texts = [doc.page_content for doc in all_documents]
    metadatas = [doc.metadata for doc in all_documents]
    
    vector_store.add_documents(documents=texts, metadatas=metadatas)
    print("\nâœ… XÃ‚Y Dá»°NG DATABASE THÃ€NH CÃ”NG!")

# ============================================================
# 5. HÃ€M TEST TRUY Váº¤N
# ============================================================
def test_search(query: str):
    print("\n" + "="*60)
    print(f"ğŸ§ª TEST TRUY Váº¤N THá»¬: \"{query}\"")
    print("="*60)
    
    if not ABS_DB_DIR.exists():
        print("âŒ Database chÆ°a Ä‘Æ°á»£c xÃ¢y dá»±ng. HÃ£y cháº¡y lá»‡nh build trÆ°á»›c.")
        return

    vector_store = VectorStoreService(
        collection_name="traffic_laws",
        persist_directory=str(ABS_DB_DIR)
    )
    
    results = vector_store.search(query, top_k=3)
    
    print(f"ğŸ” TÃ¬m tháº¥y {len(results)} káº¿t quáº£ liÃªn quan nháº¥t:\n")
    for i, res in enumerate(results, 1):
        # Láº¥y thÃ´ng tin an toÃ n
        score = res.get('similarity_score', 0)
        meta = res.get('metadata', {})
        content = res.get('document', '')
        
        # LÃ m gá»n ná»™i dung Ä‘á»ƒ hiá»ƒn thá»‹
        preview = content[:250].replace('\n', ' ') + "..."
        
        print(f"--- Top {i} (Äá»™ khá»›p: {score:.2f}) ---")
        print(f"ğŸ“Œ Nguá»“n: {meta.get('source', 'N/A')} | {meta.get('article', 'N/A')}")
        print(f"ğŸ“– Ná»™i dung: {preview}\n")

if __name__ == "__main__":
    import argparse
    
    # Cáº¥u hÃ¬nh Argument Parser
    parser = argparse.ArgumentParser(description="Tool build dá»¯ liá»‡u cho Chatbot Giao thÃ´ng")
    
    # ThÃªm argument --reset (store_true nghÄ©a lÃ  náº¿u cÃ³ cá» nÃ y thÃ¬ giÃ¡ trá»‹ lÃ  True)
    parser.add_argument("--reset", action="store_true", help="XÃ³a sáº¡ch DB cÅ© vÃ  build láº¡i tá»« Ä‘áº§u")
    
    # ThÃªm argument --test-query
    parser.add_argument("--test-query", type=str, default="khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm pháº¡t bao nhiÃªu", help="CÃ¢u há»i Ä‘á»ƒ test thá»­ sau khi build")
    
    # ThÃªm argument --skip-build
    parser.add_argument("--skip-build", action="store_true", help="Chá»‰ cháº¡y test, khÃ´ng build láº¡i DB")
    
    args = parser.parse_args()
    
    # Logic cháº¡y chÃ­nh
    if not args.skip_build:
        build_vector_database(reset=args.reset)
    
    # LuÃ´n cháº¡y test sau khi build xong (hoáº·c náº¿u skip-build)
    test_search(args.test_query)