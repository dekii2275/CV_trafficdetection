"""
RAG ChatBot Agent - Chatbot vá»›i Retrieval-Augmented Generation
Sá»­ dá»¥ng Gemini API + Vector Search Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i vá» luáº­t giao thÃ´ng
"""

import google.generativeai as genai
from typing import List, Dict, Optional, AsyncIterator
import os
import asyncio
from datetime import datetime

from app.services.rag_services.vector_store import get_vector_store


class ChatBotAgent:
    """
    RAG-powered chatbot cho tÆ° váº¥n luáº­t giao thÃ´ng Viá»‡t Nam
    """
    
    def __init__(self):
        """
        Khá»Ÿi táº¡o ChatBot vá»›i Gemini API vÃ  Vector Store
        """
        # Load API key tá»« environment
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY khÃ´ng Ä‘Æ°á»£c thiáº¿t láº­p trong environment variables")
        
        # Configure Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Khá»Ÿi táº¡o Vector Store
        print("ðŸ”„ Initializing Vector Store...")
        self.vector_store = get_vector_store()
        
        # System prompt
        self.system_prompt = """
        Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n luáº­t giao thÃ´ng Viá»‡t Nam.

        NHIá»†M Vá»¤:
        - Tráº£ lá»i chÃ­nh xÃ¡c cÃ¡c cÃ¢u há»i vá» luáº­t giao thÃ´ng dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p
        - TrÃ­ch dáº«n rÃµ rÃ ng Ä‘iá»u luáº­t, khoáº£n, Ä‘iá»ƒm liÃªn quan
        - Giáº£i thÃ­ch dá»… hiá»ƒu cho ngÆ°á»i dÃ¢n
        - Náº¿u khÃ´ng cÃ³ thÃ´ng tin trong tÃ i liá»‡u, hÃ£y nÃ³i rÃµ "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong cÃ¡c vÄƒn báº£n luáº­t hiá»‡n cÃ³"

        QUY Táº®C:
        1. LuÃ´n trÃ­ch dáº«n nguá»“n: "Theo Äiá»u X Luáº­t Y/Z/QH..."
        2. Æ¯u tiÃªn luáº­t má»›i nháº¥t náº¿u cÃ³ nhiá»u vÄƒn báº£n
        3. Cáº£nh bÃ¡o náº¿u cÃ³ thay Ä‘á»•i luáº­t gáº§n Ä‘Ã¢y
        4. ÄÆ°a ra vÃ­ dá»¥ cá»¥ thá»ƒ khi cÃ³ thá»ƒ
        5. KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin"""
        
        print("ChatBotAgent initialized successfully")
    
    async def get_response(
        self,
        message: str,
        session_id: str,
        conversation_history: Optional[List[Dict]] = None,
        top_k: int = 5
    ) -> Dict:
        try:
            # BÆ¯á»šC 1: Retrieve relevant documents
            print(f"Searching for relevant laws: '{message}'")
            search_results = self.vector_store.search(
                query=message,
                top_k=top_k
            )
            
            # BÆ¯á»šC 2: Chuáº©n bá»‹ context tá»« retrieved documents
            context = self._format_context(search_results)
            
            # BÆ¯á»šC 3: Format conversation history
            history_text = self._format_history(conversation_history) if conversation_history else ""
            
            # BÆ¯á»šC 4: Táº¡o prompt vá»›i context
            full_prompt = f"""{self.system_prompt}

            THÃ”NG TIN LUáº¬T LIÃŠN QUAN:
            {context}

            Lá»ŠCH Sá»¬ Há»˜I THOáº I:
            {history_text}

            CÃ‚U Há»ŽI: {message}

            TRáº¢ Lá»œI:
            """
            
            # BÆ¯á»šC 5: Generate response tá»« Gemini
            print("Generating response with Gemini...")
            response = await self._generate_with_gemini(full_prompt)
            
            # BÆ¯á»šC 6: Extract sources Ä‘á»ƒ tráº£ vá»
            sources = self._extract_sources(search_results)
            
            return {
                "message": response,
                "sources": sources,
                "image": None,
                "retrieved_docs": len(search_results)
            }
            
        except Exception as e:
            print(f"Error in get_response: {e}")
            return {
                "message": f"Xin lá»—i, Ä‘Ã£ xáº£y ra lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n: {str(e)}",
                "sources": [],
                "image": None
            }
    
    async def _generate_with_gemini(self, prompt: str) -> str:
        """
        Gá»i Gemini API Ä‘á»ƒ generate response
        """
        try:
            # Sá»­ dá»¥ng generate_content_async cho async operation
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            return response.text
        except Exception as e:
            print(f"Gemini API error: {e}")
            raise
    
    async def stream_response(
        self,
        message: str,
        session_id: str,
        conversation_history: Optional[List[Dict]] = None,
        top_k: int = 5
    ) -> AsyncIterator[str]:
        """
        Stream response cho WebSocket (tá»«ng chunk)
        """
        try:
            # Retrieve documents
            search_results = self.vector_store.search(query=message, top_k=top_k)
            context = self._format_context(search_results)
            history_text = self._format_history(conversation_history) if conversation_history else ""
            
            full_prompt = f"""{self.system_prompt}

                THÃ”NG TIN LUáº¬T LIÃŠN QUAN:
                {context}

                Lá»ŠCH Sá»¬ Há»˜I THOáº I:
                {history_text}

                CÃ‚U Há»ŽI: {message}

                TRáº¢ Lá»œI:
                """
            
            # Stream tá»« Gemini
            response = await asyncio.to_thread(
                self.model.generate_content,
                full_prompt,
                stream=True
            )
            
            for chunk in response:
                if chunk.text:
                    yield chunk.text
                    
        except Exception as e:
            yield f"Lá»—i: {str(e)}"
    
    def _format_context(self, search_results: List[Dict]) -> str:
        """
        Format retrieved documents thÃ nh context cho prompt
        """
        if not search_results:
            return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            metadata = result['metadata']
            doc_text = result['document']
            similarity = result['similarity_score']
            
            context_parts.append(f"""
                [TÃ i liá»‡u {i}] - Äá»™ liÃªn quan: {similarity:.2%}
                Nguá»“n: {metadata.get('law_name', 'N/A')} - Äiá»u {metadata.get('article_number', 'N/A')}
                TiÃªu Ä‘á»: {metadata.get('article_title', 'N/A')}
                Ná»™i dung:
                {doc_text}
                ---""")
        
        return "\n".join(context_parts)
    
    def _format_history(self, history: List[Dict]) -> str:
        """
        Format conversation history cho prompt
        """
        if not history:
            return "KhÃ´ng cÃ³ lá»‹ch sá»­ há»™i thoáº¡i."
        
        history_parts = []
        for msg in history[-5:]:  # Chá»‰ láº¥y 5 tin nháº¯n gáº§n nháº¥t
            role = "NgÆ°á»i dÃ¹ng" if msg['role'] == 'user' else "Trá»£ lÃ½"
            history_parts.append(f"{role}: {msg['content']}")
        
        return "\n".join(history_parts)
    
    def _extract_sources(self, search_results: List[Dict]) -> List[Dict]:
        """
        TrÃ­ch xuáº¥t thÃ´ng tin nguá»“n Ä‘á»ƒ tráº£ vá» cho client
        """
        sources = []
        for result in search_results:
            metadata = result['metadata']
            sources.append({
                "law_name": metadata.get('law_name', 'N/A'),
                "article": metadata.get('article_number', 'N/A'),
                "title": metadata.get('article_title', 'N/A'),
                "source_file": metadata.get('source_file', 'N/A'),
                "similarity": f"{result['similarity_score']:.2%}"
            })
        
        return sources
    
    def get_stats(self) -> Dict:
        """
        Láº¥y thá»‘ng kÃª vá» vector store
        """
        return self.vector_store.get_collection_info()


# Singleton instance
_agent = None

def get_agent() -> ChatBotAgent:
    """
    Get hoáº·c táº¡o ChatBotAgent instance (singleton)
    """
    global _agent
    if _agent is None:
        _agent = ChatBotAgent()
    return _agent