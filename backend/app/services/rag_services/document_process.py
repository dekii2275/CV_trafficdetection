"""
Document Processor - Xá»­ lÃ½ vÃ  chia nhá» vÄƒn báº£n luáº­t giao thÃ´ng
"""

from pathlib import Path
from typing import List, Dict, Tuple
import docx
import re


class DocumentProcessor:
    """
    Xá»­ lÃ½ vÄƒn báº£n luáº­t tá»« .doc/.docx files
    """
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 100):
        """
        Args:
            chunk_size: Äá»™ dÃ i tá»‘i Ä‘a cá»§a má»—i chunk (kÃ½ tá»±)
            chunk_overlap: Sá»‘ kÃ½ tá»± overlap giá»¯a cÃ¡c chunk
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def read_docx(self, file_path: str) -> str:
        """
        Äá»c ná»™i dung tá»« file .docx
        """
        try:
            doc = docx.Document(file_path)
            full_text = []
            for para in doc.paragraphs:
                if para.text.strip():
                    full_text.append(para.text)
            return "\n".join(full_text)
        except Exception as e:
            print(f"âŒ Error reading {file_path}: {e}")
            return ""
    
    def extract_law_sections(self, text: str) -> List[Dict[str, str]]:
        """
        TÃ¡ch vÄƒn báº£n thÃ nh cÃ¡c Ä‘iá»u luáº­t riÃªng biá»‡t
        
        Pattern:
        - Äiá»u 1. TiÃªu Ä‘á»
        - 1. Ná»™i dung Ä‘iá»ƒm 1
        - 2. Ná»™i dung Ä‘iá»ƒm 2
        """
        sections = []
        
        # Pattern Ä‘á»ƒ tÃ¬m cÃ¡c Äiá»u luáº­t
        article_pattern = r'Äiá»u\s+(\d+)[.\s]+([^\n]+)'
        
        # TÃ¬m táº¥t cáº£ cÃ¡c Äiá»u
        articles = list(re.finditer(article_pattern, text))
        
        for i, match in enumerate(articles):
            article_num = match.group(1)
            article_title = match.group(2).strip()
            
            # Láº¥y ná»™i dung tá»« Äiá»u hiá»‡n táº¡i Ä‘áº¿n Äiá»u tiáº¿p theo
            start_pos = match.end()
            end_pos = articles[i + 1].start() if i + 1 < len(articles) else len(text)
            article_content = text[start_pos:end_pos].strip()
            
            sections.append({
                "article_number": article_num,
                "title": article_title,
                "content": article_content,
                "full_text": f"Äiá»u {article_num}. {article_title}\n{article_content}"
            })
        
        return sections
    
    def chunk_text(self, text: str) -> List[str]:
        """
        Chia text thÃ nh cÃ¡c chunks nhá» vá»›i overlap
        """
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + self.chunk_size
            
            # TÃ¬m dáº¥u cÃ¢u gáº§n nháº¥t Ä‘á»ƒ khÃ´ng cáº¯t giá»¯a cÃ¢u
            if end < text_length:
                # TÃ¬m dáº¥u cÃ¢u (. ! ?) gáº§n nháº¥t
                for i in range(end, start + self.chunk_size // 2, -1):
                    if text[i] in '.!?\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.chunk_overlap
        
        return chunks
    
    def process_law_documents(
        self,
        documents_dir: str = "./data/law_documents"
    ) -> Tuple[List[str], List[Dict]]:
        """
        Xá»­ lÃ½ táº¥t cáº£ vÄƒn báº£n luáº­t trong thÆ° má»¥c
        
        Returns:
            (documents, metadatas) - Danh sÃ¡ch chunks vÃ  metadata tÆ°Æ¡ng á»©ng
        """
        documents = []
        metadatas = []
        doc_id = 0
        
        documents_path = Path(documents_dir)
        
        if not documents_path.exists():
            print(f"âš ï¸ Documents directory not found: {documents_dir}")
            return documents, metadatas
        
        # Xá»­ lÃ½ tá»«ng file .doc/.docx
        law_files = list(documents_path.glob("*.doc")) + list(documents_path.glob("*.docx"))
        
        print(f"ğŸ“„ Found {len(law_files)} law documents")
        
        for law_file in law_files:
            print(f"ğŸ”„ Processing: {law_file.name}")
            
            # Äá»c ná»™i dung
            full_text = self.read_docx(str(law_file))
            
            if not full_text:
                continue
            
            # TÃ¡ch thÃ nh cÃ¡c Äiá»u luáº­t
            sections = self.extract_law_sections(full_text)
            
            # Xá»­ lÃ½ tá»«ng Äiá»u
            for section in sections:
                # Chunk ná»™i dung náº¿u quÃ¡ dÃ i
                if len(section["full_text"]) > self.chunk_size:
                    chunks = self.chunk_text(section["full_text"])
                else:
                    chunks = [section["full_text"]]
                
                # ThÃªm vÃ o danh sÃ¡ch
                for i, chunk in enumerate(chunks):
                    documents.append(chunk)
                    metadatas.append({
                        "source_file": law_file.name,
                        "article_number": section["article_number"],
                        "article_title": section["title"],
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "law_name": self._extract_law_name(law_file.name)
                    })
                    doc_id += 1
        
        print(f"âœ… Processed {len(documents)} chunks from {len(law_files)} documents")
        
        return documents, metadatas
    
    def _extract_law_name(self, filename: str) -> str:
        """
        TrÃ­ch xuáº¥t tÃªn luáº­t tá»« filename
        VÃ­ dá»¥: "36_2024_QH15_m_444251.doc" -> "Luáº­t 36/2024/QH15"
        """
        # Remove extension
        name = filename.replace('.doc', '').replace('.docx', '')
        
        # Parse pattern: sá»‘_nÄƒm_cÆ¡quan
        parts = name.split('_')
        if len(parts) >= 3:
            return f"Luáº­t {parts[0]}/{parts[1]}/{parts[2]}"
        
        return name
    
    def categorize_content(self, text: str) -> str:
        """
        PhÃ¢n loáº¡i ná»™i dung (Ä‘á»ƒ filter khi search)
        
        Returns:
            Category: "bien_bao", "phat_nguoi", "giay_phep", "tai_xe", etc.
        """
        text_lower = text.lower()
        
        categories = {
            "bien_bao": ["biá»ƒn bÃ¡o", "biá»ƒn hiá»‡u", "tÃ­n hiá»‡u", "Ä‘Ã¨n giao thÃ´ng"],
            "phat_nguoi": ["pháº¡t", "vi pháº¡m", "xá»­ pháº¡t", "má»©c pháº¡t", "tiá»n pháº¡t"],
            "giay_phep": ["giáº¥y phÃ©p", "Ä‘Äƒng kÃ½", "Ä‘Äƒng kiá»ƒm", "báº±ng lÃ¡i"],
            "tai_xe": ["tÃ i xáº¿", "ngÆ°á»i lÃ¡i", "ngÆ°á»i Ä‘iá»u khiá»ƒn"],
            "toc_do": ["tá»‘c Ä‘á»™", "vÆ°á»£t quÃ¡", "km/h", "giá»›i háº¡n tá»‘c Ä‘á»™"],
            "an_toan": ["an toÃ n", "mÅ© báº£o hiá»ƒm", "dÃ¢y an toÃ n", "tai náº¡n"]
        }
        
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
        
        return "general"


# Test function
if __name__ == "__main__":
    processor = DocumentProcessor()
    documents, metadatas = processor.process_law_documents()
    
    print(f"\nğŸ“Š Statistics:")
    print(f"Total chunks: {len(documents)}")
    print(f"\nSample chunk:")
    print(documents[0][:200] + "...")
    print(f"\nSample metadata:")
    print(metadatas[0])